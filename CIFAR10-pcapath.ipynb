{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Lambda, Input, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD, Optimizer\n",
    "from keras.legacy import interfaces\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SGDR_keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Download & preprocess CIFAR-10 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download CIFAR-10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:100]\n",
    "y_train = y_train[:100]\n",
    "X_test = X_test[:30]\n",
    "y_test = y_test[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shape of input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onehot encode the target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(sparse=False)\n",
    "enc.fit(y_train.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = enc.transform(y_train.reshape(-1, 1))\n",
    "y_test = enc.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `ImageDataGenerator` to make `batches` and `test_bathces`.<br>\n",
    "Then make `batches` and `test_batches`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "gen = image.ImageDataGenerator()\n",
    "batches = gen.flow(X_train, y_train, batch_size=batch_size)\n",
    "test_batches = image.ImageDataGenerator().flow(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input data should be normalized before get into the model.<br>\n",
    "Compute mean and standard deviation of trainin input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_px = np.array([X_train[:,:,:,0].mean(), X_train[:,:,:,1].mean(), X_train[:,:,:,2].mean()], dtype=np.float32)\n",
    "std_px = np.array([X_train[:,:,:,0].std(), X_train[:,:,:,1].std(), X_train[:,:,:,2].std()], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function which return normalized input data.<br>\n",
    "This function will be used as an input layer of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_input(x): return (x-mean_px)/std_px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential([\n",
    "        Lambda(norm_input, input_shape=(32, 32, 3)),\n",
    "        Conv2D(32,3, activation='relu', padding='same'),\n",
    "        Conv2D(32,3, activation='relu', padding='same'),\n",
    "        MaxPooling2D(),\n",
    "        Conv2D(64,3, activation='relu', padding='same'),\n",
    "        Conv2D(64,3, activation='relu', padding='same'),\n",
    "        MaxPooling2D(),\n",
    "        Conv2D(128,3, activation='relu', padding='same'),\n",
    "        Conv2D(128,3, activation='relu', padding='same'),\n",
    "        MaxPooling2D(),\n",
    "        Conv2D(256,3, activation='relu', padding='same'),\n",
    "        Conv2D(256,3, activation='relu', padding='same'),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(10, activation='softmax')\n",
    "        ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get SGD and SGDR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd = get_model()\n",
    "model_sgdr = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set initial weight of SGD and SGDR model same to make identical initial starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model_sgd.get_weights()\n",
    "model_sgdr.set_weights(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_index = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = 'weights/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since keras record training history only once per one epoch, number of epochs should be changed if moreh history information is needed.<br>\n",
    "`get_epochs` function returns hypothetical number of epochs given real number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epochs(n_epochs): return int(n_epochs * n_batch / steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_batch` shows the number of batches in full dataset.<br>\n",
    "Since number of data sample is 60000, `n_batch` will equal to 60000 / `batch_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch = len(batches); n_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After `n_batch` iterations, model trained for one real epoch.<br>\n",
    "After `steps_per_epoch` iterations, model trained for one hypothetical epoch, therefore record training history information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = f'{weight_path}cifar10_path_hist/' + 'SGD-{epoch:02d}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, save_best_only=False, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Train SGD model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a list which will record the training history information of SGD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_hist = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`steps_per_epoch` defines number of steps for one hypothetical epoch.<br>\n",
    "Note that one real epoch is one cycle of full training data.<br>\n",
    "If `steps_per_epoch` is small, than Keras will record training history information more ofthen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model with SGD optimizer and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.1)\n",
    "model_sgd.compile(sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgd_hist.append(model_sgd.fit_generator(batches, epochs=get_epochs(50), callbacks=[checkpoint],\n",
    "                                        validation_data=test_batches, steps_per_epoch=steps_per_epoch, verbose=1))\n",
    "model_sgd.evaluate_generator(batches)[1], model_sgd.evaluate_generator(test_batches)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is over!<br>\n",
    "Save the weights of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_sgd.save_weights(f'{weight_path}cifar10-sgd{model_index}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_sgd.load_weights(f'{weight_path}cifar10-sgd{model_index}.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train SGDR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdr_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 10\n",
    "lr = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`iter_per_epoch` of SGDR defines the number of iterations of one cycle of learning rate.<br>\n",
    "If its same with `n_batch`, which is the number of iterations for one real epoch, then SGDR reset the learning rate for every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdr = SGDR_keras.SGDR(lr=lr, iter_per_epoch=n_batch)\n",
    "model_sgdr.compile(sgdr, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "filepath = f'{weight_path}cifar10_path_hist/' + 'SGDR-1-{epoch:02d}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, save_best_only=False, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgdr_hist.append(model_sgdr.fit_generator(batches, epochs=get_epochs(1), \n",
    "                                          validation_data=test_batches, steps_per_epoch=steps_per_epoch, verbose=1))\n",
    "model_sgdr.evaluate_generator(batches)[1], model_sgdr.evaluate_generator(test_batches)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdr = SGDR_keras.SGDR(lr=lr, iter_per_epoch=2*n_batch)\n",
    "model_sgdr.compile(sgdr, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "filepath = f'{weight_path}cifar10_path_hist/' + 'SGDR-2-{epoch:02d}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, save_best_only=False, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgdr_hist.append(model_sgdr.fit_generator(batches, epochs=get_epochs(2), callbacks=[checkpoint],\n",
    "                                          validation_data=test_batches, steps_per_epoch=steps_per_epoch, verbose=1))\n",
    "model_sgdr.evaluate_generator(batches)[1], model_sgdr.evaluate_generator(test_batches)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdr = SGDR_keras.SGDR(lr=lr, iter_per_epoch=4*n_batch)\n",
    "model_sgdr.compile(sgdr, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "filepath = f'{weight_path}cifar10_path_hist/' + 'SGDR-4-{epoch:02d}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, save_best_only=False, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgdr_hist.append(model_sgdr.fit_generator(batches, epochs=get_epochs(4), callbacks=[checkpoint],\n",
    "                                          validation_data=test_batches, steps_per_epoch=steps_per_epoch, verbose=1))\n",
    "model_sgdr.evaluate_generator(batches)[1], model_sgdr.evaluate_generator(test_batches)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdr = SGDR_keras.SGDR(lr=lr, iter_per_epoch=8*n_batch)\n",
    "model_sgdr.compile(sgdr, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "filepath = f'{weight_path}cifar10_path_hist/' + 'SGDR-8-{epoch:02d}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, save_best_only=False, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgdr_hist.append(model_sgdr.fit_generator(batches, epochs=get_epochs(8), callbacks=[checkpoint],\n",
    "                                          validation_data=test_batches, steps_per_epoch=steps_per_epoch, verbose=1))\n",
    "model_sgdr.evaluate_generator(batches)[1], model_sgdr.evaluate_generator(test_batches)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdr = SGDR_keras.SGDR(lr=lr, iter_per_epoch=16*n_batch)\n",
    "model_sgdr.compile(sgdr, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "filepath = f'{weight_path}cifar10_path_hist/' + 'SGDR-16-{epoch:02d}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, save_best_only=False, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgdr_hist.append(model_sgdr.fit_generator(batches, epochs=get_epochs(16), callbacks=[checkpoint],\n",
    "                                          validation_data=test_batches, steps_per_epoch=steps_per_epoch, verbose=1))\n",
    "model_sgdr.evaluate_generator(batches)[1], model_sgdr.evaluate_generator(test_batches)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdr = SGDR_keras.SGDR(lr=lr, iter_per_epoch=32*n_batch)\n",
    "model_sgdr.compile(sgdr, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "filepath = f'{weight_path}cifar10_path_hist/' + 'SGDR-32-{epoch:02d}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, save_best_only=False, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgdr_hist.append(model_sgdr.fit_generator(batches, epochs=get_epochs(32), callbacks=[checkpoint],\n",
    "                                          validation_data=test_batches, steps_per_epoch=steps_per_epoch, verbose=1))\n",
    "model_sgdr.evaluate_generator(batches)[1], model_sgdr.evaluate_generator(test_batches)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is over!<br>\n",
    "Save the weights of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_sgdr.save_weights(f'{weight_path}cifar10-sgdr{model_index}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_sgdr.load_weights(f'{weight_path}cifar10-sgdr{model_index}.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. get PCA direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 SGD PCA direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read weight files, then make all those weights into one single (num_params x num_epochs) matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del weights_array\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "for i in range(get_epochs(50)):\n",
    "    #Read each hdf5 file\n",
    "    w_path = f'{weight_path}cifar10_path_hist/SGD-{i+1:0>2}.hdf5'\n",
    "    new_model = get_model()\n",
    "    new_model.load_weights(w_path)\n",
    "    \n",
    "    #Flatten weights (do not consider BN weights)\n",
    "    weights = np.array([])\n",
    "    for l in new_model.layers:\n",
    "        w = l.get_weights()\n",
    "        if isinstance(l, Conv2D) or isinstance(l, Dense): \n",
    "            weights = np.append(weights, w[0])\n",
    "            weights = np.append(weights, w[1])\n",
    "    \n",
    "    #Append to the array\n",
    "    try:\n",
    "        weights_array = np.column_stack((weights_array, weights))\n",
    "    except NameError:\n",
    "        weights_array = weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shape of `weights_array`. This should be (number of params, number of epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then compute variation of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_delta = (weights_array - np.expand_dims(weights_array[:,4], axis=1))[:,:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do pca for `weights_delta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(weights_delta.T)\n",
    "sgd_pca_direction = np.swapaxes(pca.components_, 0, 1)\n",
    "sgd_pca_data = pca.transform(weights_array.T)\n",
    "sgd_pca_direction.shape, sgd_pca_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape `sgd_pca_diraction` to the shape of the original weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_direction(direction):\n",
    "    #make temporary model to get the shape of the weight\n",
    "    model = get_model()\n",
    "    \n",
    "    reshaped_weight = []\n",
    "    for l in model.layers:\n",
    "        w = l.get_weights()\n",
    "        if isinstance(l, Conv2D) or isinstance(l, Dense):\n",
    "            for i in range(len(w)):\n",
    "                tmp_weight = direction[:w[i].size]\n",
    "                direction = direction[w[i].size:]\n",
    "                tmp_weight = tmp_weight.reshape(w[i].shape)\n",
    "                reshaped_weight.append(tmp_weight)\n",
    "            \n",
    "        elif isinstance(l, BatchNormalization):\n",
    "            for i in range(len(w)):\n",
    "                reshaped_weight.append(np.zeros(shape = w[i].shape))\n",
    "                \n",
    "    return reshaped_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_direction1 = reshape_direction(sgd_pca_direction[:,0])\n",
    "sgd_direction2 = reshape_direction(sgd_pca_direction[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter-normalize `sgd_pca_direction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_direction(model, direction):\n",
    "    normalized_direction = []\n",
    "    \n",
    "    tmp_model = get_model()\n",
    "    tmp_model.set_weights(direction)\n",
    "    for l, tmp_l in zip(model.layers, tmp_model.layers):\n",
    "        w = l.get_weights()\n",
    "        d = tmp_l.get_weights()\n",
    "        \n",
    "        #if layer is convolutional layer\n",
    "        if isinstance(l, Conv2D):\n",
    "            #make direction array\n",
    "            filter_w = np.zeros(w[0].shape)\n",
    "            bias_w = np.zeros(w[1].shape)\n",
    "\n",
    "            for f in range(l.filters):\n",
    "                for i in range(l.input_shape[3]):\n",
    "                    #randomly generate direction\n",
    "                    temp_direction = d[0][:,:,i,f]\n",
    "                    temp_bias = d[1][f]\n",
    "\n",
    "                    #compute norm of direction and original filter\n",
    "                    norm_model = np.linalg.norm(w[0][:,:,i,f], ord='fro')\n",
    "                    norm_direction = np.linalg.norm(temp_direction, ord='fro')\n",
    "\n",
    "                    #normalize generated direction\n",
    "                    temp_direction = temp_direction / norm_direction * norm_model\n",
    "                    temp_bias = temp_bias / norm_direction * norm_model\n",
    "\n",
    "                    #put generated one-filter direction to array\n",
    "                    filter_w[:,:,i,f] = temp_direction\n",
    "                    bias_w[f] = temp_bias\n",
    "\n",
    "            #append generate one-layer direction to direction list\n",
    "            normalized_direction.append(filter_w)\n",
    "            normalized_direction.append(bias_w)\n",
    "            \n",
    "        #if layer is FC\n",
    "        elif isinstance(l, Dense):\n",
    "            \n",
    "            #randomly generate direction\n",
    "            temp_direction = d[0]\n",
    "            temp_bias = d[1]\n",
    "            \n",
    "            #compute norm of direction and original layer\n",
    "            norm_model = np.linalg.norm(w[0], ord='fro')\n",
    "            norm_direction = np.linalg.norm(temp_direction, ord='fro')\n",
    "            \n",
    "            #normalize generated direction\n",
    "            temp_direction = temp_direction / norm_direction * norm_model\n",
    "            temp_bias = temp_bias / norm_direction * norm_model\n",
    "            \n",
    "            #put generated one-layer direction to array\n",
    "            normalized_direction.append(temp_direction)\n",
    "            normalized_direction.append(temp_bias)\n",
    "            \n",
    "        #if layer is BN\n",
    "        elif isinstance(l, BatchNormalization):\n",
    "            \n",
    "            temp_direction_list = []\n",
    "            \n",
    "            #randomly generate direction\n",
    "            for i in range(len(w)):\n",
    "                temp_direction_list.append(np.zeros(w[i].shape))\n",
    "            \n",
    "            \n",
    "            #put generated one-layer direction to array\n",
    "            for d in temp_direction_list:\n",
    "                normalized_direction.append(d)\n",
    "            \n",
    "    return normalized_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_direction1 = normalize_direction(model_sgd, sgd_direction1)\n",
    "sgd_direction2 = normalize_direction(model_sgd, sgd_direction2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 SGDR PCA direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del weights_array\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "cycle_list = [1, 2, 4, 8, 16, 32]\n",
    "\n",
    "for c in cycle_list:\n",
    "    for i in range(get_epochs(1)):\n",
    "        #Read each hdf5 file\n",
    "        w_path = f'{weight_path}cifar10_path_hist/SGDR-{c}-{i+1:0>2}.hdf5'\n",
    "        new_model = get_model()\n",
    "        new_model.load_weights(w_path)\n",
    "\n",
    "        #Flatten weights (do not consider BN weights)\n",
    "        weights = np.array([])\n",
    "        for l in new_model.layers:\n",
    "            w = l.get_weights()\n",
    "            if isinstance(l, Conv2D) or isinstance(l, Dense): \n",
    "                weights = np.append(weights, w[0])\n",
    "                weights = np.append(weights, w[1])\n",
    "\n",
    "        #Append to the array\n",
    "        try:\n",
    "            weights_array = np.column_stack((weights_array, weights))\n",
    "        except NameError:\n",
    "            weights_array = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_delta = (weights_array - np.expand_dims(weights_array[:,4], axis=1))[:,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(weights_delta.T)\n",
    "sgdr_pca_direction = np.swapaxes(pca.components_, 0, 1)\n",
    "sgdr_pca_data = pca.transform(weights_array.T)\n",
    "sgdr_pca_direction.shape, sgdr_pca_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape `sgdr_pca_diraction` to the shape of the original weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdr_direction1 = reshape_direction(sgdr_pca_direction[:,0])\n",
    "sgdr_direction2 = reshape_direction(sgdr_pca_direction[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter-normalize `sgdr_pca_direction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdr_direction1 = normalize_direction(model_sgdr, sgdr_direction1)\n",
    "sgdr_direction2 = normalize_direction(model_sgdr, sgdr_direction2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Draw plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`direction_step` function returns model of which weight is alpha * direction + original_model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direction_step(direction, model, alpha):\n",
    "    \n",
    "    step_model = get_model()\n",
    "    step_model.compile(sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    #get original model weights\n",
    "    weight = model.get_weights()\n",
    "    \n",
    "    new_weights = []\n",
    "    for i, w in enumerate(weight):\n",
    "        new_weights.append(w + alpha * direction[i])\n",
    "        \n",
    "    step_model.set_weights(new_weights)\n",
    "    \n",
    "    return step_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = np.linspace(-10, 10, num=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_2d_fn(model, alpha_list, direction1, direction2):\n",
    "    loss_test, acc_test = [], []\n",
    "    \n",
    "    for a1 in tqdm(alpha_list):\n",
    "        tmp_loss, tmp_acc = [], []\n",
    "        for a2 in (alpha_list):\n",
    "            \n",
    "            temp_model = direction_step(direction1, model, a1)\n",
    "            temp_model = direction_step(direction2, temp_model, a2)\n",
    "        \n",
    "            eval_test = temp_model.evaluate_generator(test_batches)\n",
    "            del temp_model\n",
    "        \n",
    "            tmp_loss.append(eval_test[0])\n",
    "            tmp_acc.append(eval_test[1])\n",
    "            \n",
    "        loss_test.append(tmp_loss)\n",
    "        acc_test.append(tmp_acc)\n",
    "        \n",
    "    return loss_test, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_sgd_test, acc_sgd_test = step_2d_fn(model_sgd, alpha_list, sgd_direction1, sgd_direction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(f'{path}cifar10-loss_sgd_test.npy', np.array(loss_sgd_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_sgd_test=np.load(f'{path}cifar10-loss_sgd_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_sgdr_test, acc_sgdr_test = step_2d_fn(model_sgdr, alpha_list, sgdr_direction1, sgdr_direction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(f'{path}cifar10-loss_sgdr_test.npy', np.array(loss_sgdr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_sgdr_test=np.load(f'{path}cifar10-loss_sgdr_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "c = plt.contour(alpha_list, alpha_list, loss_sgd_test)\n",
    "plt.clabel(c, inline=1, fontsize=10)\n",
    "plt.title('SGD test countour plot', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "c = plt.contour(alpha_list, alpha_list, loss_sgdr_test)\n",
    "plt.clabel(c, inline=1, fontsize=10)\n",
    "plt.title('SGDR test countour plot', fontsize=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
